The image illustrates a four-step workflow for building and maintaining a **Prompt Library integrated with Jira**. In Step 1, developers create a **custom field in Jira** to capture prompts related to their tasks, stories, or bugs, ensuring every work item can store valuable prompt data. Step 2 shows that while working on a Jira task, the developer can **search an existing Prompt Library**, such as through a tool like Devin, to find relevant prompts that boost productivity and reuse proven solutions. The visual also indicates that tracking prompt usage helps measure the **Prompt Hit Rate** over time, reflecting efficiency gains. In Step 3, after completing or refining a task, the developer **records the effective prompt in Jira** within the custom field. Finally, Step 4 involves a **weekly export of captured prompts** from Jira into a **centralized database or SharePoint folder**, ensuring knowledge retention and accessibility. This cyclical process allows continuous improvement of the prompt library. The **Prompt Library** acts as a growing knowledge base that captures team-specific best practices. Over time, it reduces redundant effort, accelerates development, and ensures consistent output quality. Additionally, this system supports collaboration and onboarding by giving new developers immediate access to tested prompts. It also enables prompt performance tracking and insights into which prompts deliver the highest impact. Having such a structured library ultimately improves **developer efficiency, innovation, and cross-project consistency**.



Developers begin by selecting a Jira story or ticket based on sprint planning and performing a high-level impact analysis to understand the requirement. Once the scope is clear, they decide on the level of AI involvement depending on the taskâ€™s complexity. If the task can be fully automated, the developer leverages an AI companion such as Devin to decompose the task and construct detailed prompts with all necessary context, constraints, and examples. The AI companion then analyzes the requirement, proposes a solution plan, and performs code changes, testing, and pull request (PR) creation. The PR is reviewed manually and verified by an AI review agent to ensure adherence to standards, best practices, and to identify potential side effects. The AI companion can also act on review feedback, refine the code, and resubmit the PR for approval.

For tasks that require manual coding with AI assistance, developers perform code changes based on the impact analysis while collaborating with tools like GitHub Copilot to enhance code quality and create unit test cases. After testing, they raise a PR which undergoes the same review and verification process as the fully automated workflow. Once the PR is approved, the code is deployed to the QA environment where manual or automated testing takes place, and any identified bugs are logged for resolution. QA signoff occurs after all issues are resolved and tested, initiating the UAT release process. Finally, developers use AI companions to update documentation, knowledge bases, and playbooks based on new learnings, maintaining continuous improvement.

The key highlights of this process are the seamless integration of AI companions throughout the development lifecycle, dual-mode flexibility (fully automated or AI-assisted), and an iterative feedback loop that strengthens quality assurance and knowledge reuse. This approach enhances productivity, enforces coding consistency, and ensures that learnings continuously refine future development cycles.

